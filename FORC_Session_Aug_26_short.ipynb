{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB_il5ojfrD_"
      },
      "outputs": [],
      "source": [
        "pip install portkey_ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WknQJq7UivHk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpCbEEEYfrEC"
      },
      "outputs": [],
      "source": [
        "from portkey_ai import Portkey\n",
        "\n",
        "portkey = Portkey(\n",
        "  api_key = userdata.get('PORT_KEY')\n",
        ")\n",
        "\n",
        "response = portkey.chat.completions.create(\n",
        "    model = \"@first-integrati-db9427/gemini-2.0-flash-lite\",\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"What is Portkey\"}\n",
        "    ],\n",
        "    MAX_TOKENS = 512\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUUYq4x6frEE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read CSV files (adjust file paths to where your CSVs are saved)\n",
        "df_14_15 = pd.read_csv(\"df_14_15.csv\") #change PATH is required\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwZLid9xfrEF"
      },
      "outputs": [],
      "source": [
        "pd.reset_option(\"display.max_colwidth\")\n",
        "pd.reset_option(\"display.max_columns\")\n",
        "df_14_15.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HG4CxPEfrEF"
      },
      "outputs": [],
      "source": [
        "from portkey_ai import Portkey\n",
        "import time\n",
        "\n",
        "modern_titles = []\n",
        "modern_texts = []\n",
        "\n",
        "delay_between_requests = 1  # seconds\n",
        "\n",
        "# Loop through only the first 10 rows\n",
        "for idx, row in df_14_15.head(10).iterrows():\n",
        "    # Prompt for title\n",
        "    title_prompt = (\n",
        "        \"Translate the following historical book title from the 1400–1500s into modern English. \"\n",
        "        \"Do not add any explanation, only return the translated title:\\n\\n\"\n",
        "        f\"'{row['Title']}'\"\n",
        "    )\n",
        "\n",
        "    # Prompt for full text\n",
        "    text_prompt = (\n",
        "        \"Translate the following passage from Early Modern English (1400–1500s) into clear, modern English prose. \"\n",
        "        \"Retain the original meaning and tone as much as possible. Only return the translated passage:\\n\\n\"\n",
        "        f\"{row['Text']}\"\n",
        "    )\n",
        "\n",
        "    # Translate title\n",
        "    try:\n",
        "        response_title = portkey.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": title_prompt}],\n",
        "            model=\"@first-integrati-db9427/gemini-2.0-flash-lite\",\n",
        "            max_completion_tokens=1000\n",
        "        )\n",
        "        modern_titles.append(response_title.choices[0].message.content.strip())\n",
        "    except Exception as e:\n",
        "        modern_titles.append(f\"[Error in title: {str(e)}]\")\n",
        "\n",
        "    time.sleep(delay_between_requests)\n",
        "\n",
        "    # Translate text\n",
        "    try:\n",
        "        response_text = portkey.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": text_prompt}],\n",
        "            model=\"@first-integrati-db9427/gemini-2.0-flash-lite\",\n",
        "            max_completion_tokens=1000\n",
        "        )\n",
        "        modern_texts.append(response_text.choices[0].message.content.strip())\n",
        "    except Exception as e:\n",
        "        modern_texts.append(f\"[Error in text: {str(e)}]\")\n",
        "\n",
        "    print(f\"Processed row {idx + 1}/10\")\n",
        "    time.sleep(delay_between_requests)\n",
        "\n",
        "\n",
        "first10 = df_14_15.head(10).index\n",
        "df_14_15.loc[first10, \"modern_title\"] = modern_titles\n",
        "df_14_15.loc[first10, \"modern_text\"]  = modern_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKSCF5jy5uEC"
      },
      "source": [
        "Personal Project Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16x82zPm5rmD"
      },
      "outputs": [],
      "source": [
        "from portkey_ai import Portkey\n",
        "import time\n",
        "\n",
        "modern_titles = []\n",
        "modern_texts = []\n",
        "\n",
        "delay_between_requests = 1  # seconds\n",
        "\n",
        "# Loop through only the first 10 rows\n",
        "for idx, row in df_14_15.head(10).iterrows():\n",
        "    # Prompt for title\n",
        "    title_prompt = (\n",
        "        \"Translate the following historical book title from the 1400–1500s into modern English. \"\n",
        "        \"Do not add any explanation, only return the translated title:\\n\\n\"\n",
        "        f\"'{row['Title']}'\"\n",
        "    )\n",
        "\n",
        "    # Prompt for full text\n",
        "    text_prompt = (\n",
        "        \"Translate the following passage from Early Modern English (1400–1500s) into clear, modern English prose. \"\n",
        "        \"Retain the original meaning and tone as much as possible. Only return the translated passage:\\n\\n\"\n",
        "        f\"{row['Text']}\"\n",
        "    )\n",
        "\n",
        "    # Translate title\n",
        "    try:\n",
        "        response_title = portkey.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": title_prompt}],\n",
        "            model=\"@first-integrati-db9427/gemini-2.0-flash-lite\",\n",
        "            max_completion_tokens=1000\n",
        "        )\n",
        "        modern_titles.append(response_title.choices[0].message.content.strip())\n",
        "    except Exception as e:\n",
        "        modern_titles.append(f\"[Error in title: {str(e)}]\")\n",
        "\n",
        "    time.sleep(delay_between_requests)\n",
        "\n",
        "    # Translate text\n",
        "    try:\n",
        "        response_text = portkey.chat.completions.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": text_prompt}],\n",
        "            model=\"@first-integrati-db9427/gemini-2.0-flash-lite\",\n",
        "            max_completion_tokens=1000\n",
        "        )\n",
        "        modern_texts.append(response_text.choices[0].message.content.strip())\n",
        "    except Exception as e:\n",
        "        modern_texts.append(f\"[Error in text: {str(e)}]\")\n",
        "\n",
        "    print(f\"Processed row {idx + 1}/10\")\n",
        "    time.sleep(delay_between_requests)\n",
        "\n",
        "\n",
        "first10 = df_14_15.head(10).index\n",
        "df_14_15.loc[first10, \"modern_title\"] = modern_titles\n",
        "df_14_15.loc[first10, \"modern_text\"]  = modern_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBp-QV1VfrEG"
      },
      "outputs": [],
      "source": [
        "first10 = df_14_15.head(10).index\n",
        "df_14_15.loc[first10, \"modern_title\"] = modern_titles\n",
        "df_14_15.loc[first10, \"modern_text\"]  = modern_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGgZNFHmfrEH"
      },
      "outputs": [],
      "source": [
        "# Show full column content (no truncation)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Show all columns if you have many\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "df_14_15.head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUUP7LL_wrD3"
      },
      "outputs": [],
      "source": [
        "#Save df as a new csv\n",
        "df_14_15.to_csv(\"df_14_15_updated.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L93RZYI7frEH"
      },
      "source": [
        "# For Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyC90XiHfrEI"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords if not already\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Combine modern_title and modern_text into one big string\n",
        "all_text = \" \".join(\n",
        "    df_14_15[\"modern_title\"].dropna().astype(str) + \" \" +\n",
        "    df_14_15[\"modern_text\"].dropna().astype(str)\n",
        ")\n",
        "\n",
        "# Tokenize: keep only words (letters/numbers)\n",
        "tokens = re.findall(r\"\\b\\w+\\b\", all_text.lower())\n",
        "\n",
        "# Remove stopwords\n",
        "tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "# Count most common words\n",
        "word_counts = Counter(tokens).most_common(20)\n",
        "\n",
        "# Plot\n",
        "words, counts = zip(*word_counts)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(words, counts)\n",
        "plt.xticks(rotation=45)\n",
        "plt.title(\"Top 20 Most Frequent Words (Translated Columns)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrGvrADqfrEJ"
      },
      "source": [
        "# RAG with this data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN2OuSBwfrEJ"
      },
      "outputs": [],
      "source": [
        "##For RAG\n",
        "!pip install pandas faiss-cpu sentence-transformers portkey-ai streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PHXkKJhfrEJ"
      },
      "outputs": [],
      "source": [
        "#To Build Index\n",
        "import json\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "CSV_PATH = \"df_14_15_updated.csv\" #change Path to where your CSV is\n",
        "META_PATH  = \"eebo_1415_meta.json\"\n",
        "INDEX_PATH = \"eebo_1415_allmini.cosine.faiss\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df = df[[\"modern_title\", \"modern_text\"]].dropna(subset=[\"modern_text\"]).reset_index(drop=True)\n",
        "\n",
        "def chunk_text(t, size=800, overlap=100):\n",
        "    t = str(t)\n",
        "    if len(t) <= size:\n",
        "        return [t]\n",
        "    chunks, start = [], 0\n",
        "    while start < len(t):\n",
        "        end = start + size\n",
        "        chunks.append(t[start:end])\n",
        "        if end >= len(t): break\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "docs, meta = [], []\n",
        "for i, row in df.iterrows():\n",
        "    title = (row[\"modern_title\"] or \"\")\n",
        "    text  = str(row[\"modern_text\"])\n",
        "    for j, ch in enumerate(chunk_text(text)):\n",
        "        docs.append((title + \"\\n\\n\" + ch).strip())\n",
        "        meta.append({\"row_id\": int(i), \"chunk_id\": int(j), \"title\": title})\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embs = model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "dim = embs.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)  # cosine if vectors are normalized\n",
        "index.add(embs)\n",
        "\n",
        "faiss.write_index(index, INDEX_PATH)\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f)\n",
        "\n",
        "print(f\"Built index with {len(docs)} chunks → {INDEX_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxpSKG7mfrEK"
      },
      "outputs": [],
      "source": [
        "#To chat using LLM API's from Portkey\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from portkey_ai import Portkey\n",
        "\n",
        "#INDEX_PATH = \"eebo_1415.faiss\"\n",
        "INDEX_PATH = \"eebo_1415_allmini.cosine.faiss\"\n",
        "META_PATH  = \"eebo_1415_meta.json\"\n",
        "CSV_PATH   = \"df_14_15_updated.csv\" #add in your PATH\n",
        "\n",
        "# --- Load retrieval artifacts ---\n",
        "index = faiss.read_index(INDEX_PATH)\n",
        "with open(META_PATH) as f:\n",
        "    META = json.load(f)\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)[[\"modern_title\", \"modern_text\"]]\n",
        "\n",
        "# Embedder (must match build_index.py)\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def retrieve(query, k=4, max_ctx_chars=2000):\n",
        "    q = embedder.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    D, I = index.search(q, k)\n",
        "    hits = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        m = META[idx]\n",
        "        row = df.iloc[m[\"row_id\"]]\n",
        "        context = (str(row[\"modern_title\"]) + \"\\n\\n\" + str(row[\"modern_text\"])).strip()\n",
        "        hits.append({\n",
        "            \"score\": float(score),\n",
        "            \"row_id\": m[\"row_id\"],\n",
        "            \"chunk_id\": m[\"chunk_id\"],\n",
        "            \"title\": m[\"title\"],\n",
        "            \"context\": context[:max_ctx_chars]\n",
        "        })\n",
        "    return hits\n",
        "\n",
        "# --- Portkey client (your format) ---\n",
        "portkey = Portkey(\n",
        "  api_key = userdata.get('PORT_KEY')\n",
        ")\n",
        "\n",
        "MODEL = \"@first-integrati-db9427/gemini-2.0-flash-lite\"\n",
        "\n",
        "def call_llm_portkey(system_prompt, user_prompt):\n",
        "    response = portkey.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def answer(query):\n",
        "    contexts = retrieve(query, k=4)\n",
        "    ctx_block = \"\\n\\n---\\n\".join(\n",
        "        [f\"[{i+1}] Title: {c['title']}\\n{c['context']}\" for i, c in enumerate(contexts)]\n",
        "    )\n",
        "    cites = \"\\n\".join([f\"- [{i+1}] row_id={c['row_id']} chunk={c['chunk_id']} title={c['title']}\"\n",
        "                       for i, c in enumerate(contexts)])\n",
        "    system = (\n",
        "        \"You are a helpful assistant. Use ONLY the provided CONTEXT to answer.\\n\"\n",
        "        \"If insufficient, say you don't know.\"\n",
        "    )\n",
        "    user = f\"QUESTION: {query}\\n\\nCONTEXT:\\n{ctx_block}\\n\\nReturn a concise answer\"\n",
        "    out = call_llm_portkey(system, user)\n",
        "    return out, cites\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"RAG chat ready (Portkey). Type a question (Ctrl+C to quit).\")\n",
        "    while True:\n",
        "        try:\n",
        "            q = input(\"\\nYou: \").strip()\n",
        "            if not q:\n",
        "                continue\n",
        "            ans, src = answer(q)\n",
        "            print(\"\\nAssistant:\", ans)\n",
        "            print(\"\\nSources:\\n\", src)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nBye!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH2dpQgZWA7G"
      },
      "source": [
        "Complaint Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RdBPz3mwzA9K"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def narrative(r):\n",
        "    date = f\"On {r['incident_date']}\" if r['incident_date'] != \"Unlisted\" else \"On an unknown date\"\n",
        "    if r['first_name'] != \"Unlisted\" and r['last_name'] != \"Unlisted\":\n",
        "        on = f\", {r['complaint_id']} was filed against officer {r['first_name']}, {r['last_name']}\"\n",
        "    else:\n",
        "        on = \", against an unknown officer\"\n",
        "    where= f\" at a(n) {r['location_type']}.\" if r['location_type'] != \"Unlisted\" else \" at an unknown location.\"\n",
        "    reason =  f\" for the reason: \\\"{r['contact_reason']}\\\".\"\n",
        "    alleg = f\" The allegation was classified with the FADO type of {r['fado_type']} with a specific allegation of {r['allegation_cat']}.\"\n",
        "    outcome = f\" The contact outcome was {r['contact_outcome']}.\"\n",
        "    ccrb_disposition = f\"The ccrb_disposition is \\\"{r['ccrb_disposition']}\\\".\" if r['ccrb_disposition'] != \" Complainant Unavailable\" else \" The ccrb_disposition is unavailable.\"\n",
        "    penalty = f\" The penalty received is {r['penalty_rec']}.\" if r['penalty_rec'] != \"Not Applicable\" else \" There was no penalty received.\"\n",
        "    status_cat = f\" The complaint status is {r['status_cat']} as of 4/1/2021.\" if r['status_cat'] != \"Unlisted\" else \" The complaint status is unknown\"\n",
        "    overview = date + on + where + reason + alleg + outcome + ccrb_disposition + penalty + status_cat;\n",
        "\n",
        "    officer_race = f\"The officer race is {r['officer_race']}.\" if r['officer_race'] != \"Unlisted\" else \"The officer race is unknown.\"\n",
        "    officer_sex = f\" The officer sex is {r['officer_gender']}.\" if r['officer_gender'] != \"Unlisted\" else \" The officer sex is unknown.\"\n",
        "    days_on_force = f\" The officer was on force for {r['days_on_force']} days when this dataset was last recorded.\" if r['days_on_force'] != \"Unlisted\" else \" The officer was on force for an unknown number of days when this dataset was last recorded.\"\n",
        "    officer_incident_rank = f\" The officer rank at the time of the incident was {r['officer_incident_rank']}.\" if r['officer_incident_rank'] != \"Unlisted\" else \" The officer rank during the incident is unknown.\"\n",
        "    officer_current_rank = f\" The officer current rank is {r['officer_current_rank']}.\" if r['officer_current_rank'] != \"Unlisted\" else \" The officer current rank is unknown.\"\n",
        "    officer_statistics = officer_race + officer_sex + days_on_force + officer_incident_rank + officer_current_rank;\n",
        "\n",
        "    impacted_race = f\"The race of the victim / alleged victim is {r['impacted_race']}\" if r['impacted_race'] != \"Unlisted\" else \"The race of the victim / alleged victim is unknown.\"\n",
        "    impacted_gender = f\" The gender of the victim / alleged victim is {r['impacted_gender']}\" if r['impacted_gender'] != \"Unlisted\" else \" The gender of the victim / alleged victim is unknown.\"\n",
        "    impacted_statistics = impacted_race + impacted_gender;\n",
        "\n",
        "    summary = (\n",
        "    f\"{overview}\\n\\n\"\n",
        "    f\"{officer_statistics}\\n\\n\"\n",
        "    f\"{impacted_statistics}\"\n",
        "    )\n",
        "    return summary\n",
        "\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "df = pd.read_csv(\"complaintclean.csv\")\n",
        "df = df.head()\n",
        "summary = df.apply(narrative, axis=1)\n",
        "summary\n",
        "#print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0682553"
      },
      "outputs": [],
      "source": [
        "# Corrected code to assign the summary to a new column\n",
        "df['summary'] = summary\n",
        "\n",
        "# Display the first few rows with the new 'summary' column\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLWlMZyy8fw_"
      },
      "outputs": [],
      "source": [
        "# Read CSV files (adjust file paths to where your CSVs are saved)\n",
        "df.to_csv(\"complaintclean_narrative.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHvBFI7D5DAN",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "##For RAG\n",
        "!pip install pandas faiss-cpu sentence-transformers portkey-ai streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkZWHyd770bZ"
      },
      "outputs": [],
      "source": [
        "#To Build Index\n",
        "import json\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "CSV_PATH = \"complaintclean_narrative.csv\" #change Path to where your CSV is\n",
        "META_PATH  = \"eebo_cc_meta.json\"\n",
        "INDEX_PATH = \"eebo_cc_allmini.cosine.faiss\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df = df[[\"complaint_id\", \"summary\"]].dropna(subset=[\"summary\"]).reset_index(drop=True)\n",
        "\n",
        "def chunk_text(t, size=800, overlap=100):\n",
        "    t = str(t)\n",
        "    if len(t) <= size:\n",
        "        return [t]\n",
        "    chunks, start = [], 0\n",
        "    while start < len(t):\n",
        "        end = start + size\n",
        "        chunks.append(t[start:end])\n",
        "        if end >= len(t): break\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "docs, meta = [], []\n",
        "for i, row in df.iterrows():\n",
        "    title = str((row[\"complaint_id\"] or \"\"))\n",
        "    text  = str(row[\"summary\"])\n",
        "    for j, ch in enumerate(chunk_text(text, size=200)):\n",
        "        docs.append((title + \"\\n\\n\" + ch).strip())\n",
        "        meta.append({\"row_id\": int(i), \"chunk_id\": int(j), \"title\": title})\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "embs = model.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "dim = embs.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)  # cosine if vectors are normalized\n",
        "index.add(embs)\n",
        "\n",
        "faiss.write_index(index, INDEX_PATH)\n",
        "with open(META_PATH, \"w\") as f:\n",
        "    json.dump(meta, f)\n",
        "\n",
        "print(f\"Built index with {len(docs)} chunks → {INDEX_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmnexnep_woR"
      },
      "outputs": [],
      "source": [
        "#To chat using LLM API's from Portkey\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from portkey_ai import Portkey\n",
        "from google.colab import userdata\n",
        "\n",
        "#INDEX_PATH = \"eebo_1415.faiss\"\n",
        "INDEX_PATH = \"eebo_cc_allmini.cosine.faiss\"\n",
        "META_PATH  = \"eebo_cc_meta.json\"\n",
        "CSV_PATH   = \"complaintclean_narrative.csv\" #add in your PATH\n",
        "\n",
        "# --- Load retrieval artifacts ---\n",
        "index = faiss.read_index(INDEX_PATH)\n",
        "with open(META_PATH) as f:\n",
        "    META = json.load(f)\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)[[\"complaint_id\", \"summary\"]]\n",
        "\n",
        "# Embedder (must match build_index.py)\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "def retrieve(query, k=4, max_ctx_chars=2000):\n",
        "    q = embedder.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    D, I = index.search(q, k)\n",
        "    hits = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        m = META[idx]\n",
        "        row = df.iloc[m[\"row_id\"]]\n",
        "        context = (str(row[\"complaint_id\"]) + \"\\n\\n\" + str(row[\"summary\"])).strip()\n",
        "        hits.append({\n",
        "            \"score\": float(score),\n",
        "            \"row_id\": m[\"row_id\"],\n",
        "            \"chunk_id\": m[\"chunk_id\"],\n",
        "            \"title\": m[\"title\"],\n",
        "            \"context\": context[:max_ctx_chars]\n",
        "        })\n",
        "    return hits\n",
        "\n",
        "# --- Portkey client (your format) ---\n",
        "portkey = Portkey(\n",
        "  api_key = userdata.get('PORT_KEY')\n",
        ")\n",
        "\n",
        "MODEL = \"@first-integrati-db9427/gemini-2.0-flash-lite\"\n",
        "\n",
        "def call_llm_portkey(system_prompt, user_prompt):\n",
        "    response = portkey.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ],\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def answer(query):\n",
        "    contexts = retrieve(query, k=2)\n",
        "    ctx_block = \"\\n\\n---\\n\".join(\n",
        "        [f\"[{i+1}] Title: {c['title']}\\n{c['context']}\" for i, c in enumerate(contexts)]\n",
        "    )\n",
        "    cites = \"\\n\".join([f\"- [{i+1}] row_id={c['row_id']} chunk={c['chunk_id']} title={c['title']}\"\n",
        "                       for i, c in enumerate(contexts)])\n",
        "    system = (\n",
        "        \"You are a helpful assistant. Use the provided CONTEXT to answer.\\n\"\n",
        "        \"but if you need to, use verified external knowledge and make a disclaimer to let\"\n",
        "        \"the user know that you're using external data\"\n",
        "        \"Don't be afraid to summarize all of the data\"\n",
        "        \"If insufficient, say you don't know.\"\n",
        "    )\n",
        "    user = f\"QUESTION: {query}\\n\\nCONTEXT:\\n{ctx_block}\\n\\nReturn a concise answer\"\n",
        "    out = call_llm_portkey(system, user)\n",
        "    return out, cites\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"RAG chat ready (Portkey). Type a question (Ctrl+C to quit).\")\n",
        "    while True:\n",
        "        try:\n",
        "            q = input(\"\\nYou: \").strip()\n",
        "            if not q:\n",
        "                continue\n",
        "            ans, src = answer(q)\n",
        "            print(\"\\nAssistant:\", ans)\n",
        "            print(\"\\nSources:\\n\", src)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nBye!\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (video)",
      "language": "python",
      "name": "video"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "758b54b07f4628f2981a93ec2fa893cf8f1006e199b2e47f7b9610a507fb2008"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}